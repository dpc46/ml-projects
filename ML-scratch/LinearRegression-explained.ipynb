{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Walk along"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "1. Aim to find best straight line fit for given data (using y = wx + b)\n",
    "2. Use the Mean Squared Error cost function to evaluate given (w, b)\n",
    "3. Use the Gradient Descent method to iterate through (w, b) to find the optimised parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Squared Error (MSE)\n",
    "\n",
    "The error is simply the true value, y, less the predicted value, y hat.\n",
    "We then square the error and take the mean of these errors\n",
    "(i.e. sum over all N errors and divide by N)\n",
    "\n",
    "$$ MSE = \\frac{1}{N}\\sum^{N}_{i = 1} (y - \\hat{y})^2$$\n",
    "$$= \\frac{1}{N}\\sum^{N}_{i = 1} (y - (wx + b))^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Method\n",
    "\n",
    "Let  $MSE = J(w, b)$ such that the gradient of the of the MSE is $ J^\\prime (w, b) $. Using the chain rule we can see for a given function $ \\hat{y} $:\n",
    "\n",
    "$$  J^\\prime (w, b) = \n",
    "\\huge \\begin{bmatrix} \\frac{d\\hat{y}}{dw} \\\\ \\frac{d\\hat{y}}{db} \\end{bmatrix}\n",
    "= \\large \\begin{bmatrix} \\frac{1}{N} \\sum -2x_i(y-\\hat{y}) \\\\ \\frac{1}{N} \\sum -2(y-\\hat{y}) \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We take some initial weights $ (w_0, b_0) $ and calculate $ J(w_0, b_0)$. We then seek to define a more appropriate (w, b), which we do by moving down the path of steepest descent (equal to the derivative of the function). \n",
    "\n",
    "We may note that here $ J^\\prime (w, b) $ is nothing other than $ \\nabla \\vec{J} $\n",
    "\n",
    "Hence, we define our update rules:\n",
    "$$ \n",
    "\\vec{J}(w_{i+1}, b_{i+1}) = \\vec{J}(w_i, b_i) - \\alpha \\nabla \\vec{J}(w_i, b_i) \n",
    "$$\n",
    "$$\n",
    "i.e. \\begin{bmatrix} w_{i+1} \\\\ b_{i+1} \\end{bmatrix} = \\begin{bmatrix} w_i - \\alpha \\cdot dw_i \\\\ b_i - \\alpha \\cdot db_i \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The parameter $ \\alpha $ is the 'learning rate' of the method and is important in defining the rate of the and accuracy of the descent. A large alpha may result in the method diverging, whereas a small learning rate may be computationally costly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Implementation\n",
    "\n",
    "Since the gradient descent method is iterative, we code this using a for loop.\n",
    "Although the example we used fits only a straightv line, we have allowed for the x input to be a vector $ \\vec{x} $\n",
    "\n",
    "This means careful treatment is needed when implementing the $\\sum -2x_i(y-\\hat{y})$ term, since this should actually be done with $x_i^T$\n",
    "\n",
    "```\n",
    "for _ in range(self.n_iters):\n",
    "\n",
    "    y_predicted = np.dot(X, self.weights) + self.bias\n",
    "\n",
    "    dw = 2 * (1/n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "    db = 2 * (1/n_samples) * np.sum(y_predicted - y)\n",
    "\n",
    "    self.weights -= self.lr * dw\n",
    "    self.bias -= self.lr * db\n",
    "\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "58e202cc2742429161fa37282d912166af2d8a9c86272d39640ddd297c4a8613"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
