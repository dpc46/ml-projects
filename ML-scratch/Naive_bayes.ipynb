{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "Naive Bayes is so called because we use Bayes theorem, but assume that all the input variables are independent (such as the weather being sunny and the day of the week being Wednesday). It is a classification model.\n",
    "\n",
    "### Bayes Theorem\n",
    "\n",
    "Take time to consider Bayes theorem more closely. Consider two classes: classical and pop. Imagining these are the only two genres of music, we might assume that out of 100 streams, 80 are pop and only 20 are classical, i.e. $ P(H) = 0.2$, $P(¬H) = 0.8 $ where we take $ P(H) $ to be the probability of a given stream of music being classical.\n",
    "\n",
    "We then consider the fact that this given music stream is being played in a coffee shop. We might assume that in a coffee shop the chance that the music being played is classical is only 1 in 10 and so the fact we know that the music is being played in a coffee shop restricts the probability space, altering the chance the music being played is classical.\n",
    "\n",
    "Without context, the expected number of classical streams would be 20 in 100, but adding in that the location is a coffee shop restricts us, because only 2 in 20 coffee shops play classical. We then consider without context, we would expect 80 of 100 streams to be not classical, jumping up to 72 in 80 coffee shops. \n",
    "\n",
    "We then consider the restricted coffee space (noting the probability of there being not classical music playing in a coffee shop must be stricly less than probability of there being a coffee shop in all but the trivial case). Intuitively, we know that $P(H|E) < P(H|¬E) $, i.e., given that we're in a coffee shop, we're less likely to hear classic than normal\n",
    "\n",
    "$$\n",
    "P(H|E) = \\frac{2}{72 + 2} = \\frac{P(H|E)}{P(H|E) + P(¬H|E)} = \\frac{P(H|E)}{P(E)}\n",
    "$$\n",
    "\n",
    "\n",
    "Formally, this is often written:\n",
    "$$\n",
    "P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Classifier\n",
    "\n",
    "Let the possible k output classes be represented by $C_k$, so that the $i^{th}$ class is represented by $C_i$\n",
    "Let also there be n features (independent variables), such that $\\bold{x} = (x_1, x_2, x_3, \\dots, x_n)$\n",
    "\n",
    "We then consider the probability of a given class for a given input vector. In mathematical notation:\n",
    "$$\n",
    "p(C_k|\\bold{x})\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "adee53e5d0909127477016e136e211551a7782b3a726cd149ece7b9d1777d1b7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
