{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Layer Perceptron\n",
    "\n",
    "The single layer perceptron takes in $ n $ inputs:\n",
    "1. Applies a bias to each\n",
    "2. Sums each of the weighted inputs\n",
    "3. Adds a bias to the sum\n",
    "4. Introduces an activation function to give an output\n",
    "\n",
    "We can use the linear model. Considering the inputs as a single $ n \\times 1 $ input vector $\\vec{x}$, we can multiply the vector by the transposed weights vector $ \\vec{w} $ and add a bias (scalar). I.e.:\n",
    "\n",
    "$$\n",
    "f(w, b) = \\vec{w}^T\\vec{x} + b\n",
    "$$\n",
    "\n",
    "### Activation Function\n",
    "The function $ f(w, b) $ then has an activation applied to it. There are many activation functions (linear, softmax, Relu, Sigmoid). We will use the very simplistic step-function here (which has biological inspiration, beyond just being simplistic, as neurons in the brain and either \"on\" or \"off\")\n",
    "\n",
    "$$\n",
    "  g(\\bold{z})=\\begin{cases}\n",
    "    1, & \\text{if $\\bold{z} \\ge 0$}.\\\\\n",
    "    0, & \\text{otherwise}.\n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "More generally, the function may be greater than a contant, but given we are adding a bias here, this is irrelevant."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "adee53e5d0909127477016e136e211551a7782b3a726cd149ece7b9d1777d1b7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
